# 大数据

## 大数据分析







## 大数据开发

### Hadoop大数据平台架构与实践课程简介

https://www.imooc.com/video/7642

大数据技术的相关概念
Hadoop的架构和运行机制

实战Hadoop的安装和配置
Hadoop开发

### 预备基础知识

```
Linux基础知识
Java编程基础
```

### 推荐书籍

```
Hadoop权威指南
Hadoop技术详解
```

### 解决大数据技术的难点

```
google大数据技术
MapReduce、BigTable、GFS
```

### Hadoop生态

#### hive 

sql语句转化为Hadoop任务

#### hbase 

和传统关系型数据库的区别是hbase放弃了事务特性，追求更高的扩展。

和hdfs的区别是提供数据的随机读写和实时访问，实现对表数据的读写功能

#### zookeeper

维护结点

### 版本选择

1.2更稳定
2.x版本变动较大
暂时选择1.2版本进行学习

### 安装Hadoop

#### 获取一个Linux系统

租用云主机
阿里云
腾讯云
unitedstack

#### 安装jdk

#### 安装Hadoop



### 开发语言与生态

| 技术框架 | 开发语言 |
| -------- | :------- |
| hadoop   | java     |
| hive     | java     |
| storm    | clojure  |
| kafka    | scala    |
| spark    | scala    |
| flink    | scala    |

### 学习地址

> https://www.imooc.com/video/16284

### Hadoop平台

#### 课程简介

- 理论知识

```
大数据的相关概念及行业生态
Hadoop的基础原理及其架构
```

- 应用知识

```
实战：hdfs实际操作
```







## 企业级大数据框架

#### 数据收集

## 企业级大数据技术体系的概述

数据收集层

数据存储层

资源管理与服务协调层

计算引擎层

数据分析层

数据可视化层

## 关系数据的收集

sqoop
可以记忆为sql+hadoop
解决关系型数据库与Hadoop之间的数据传输问题

```
数据迁移
可视化分析结果
数据增量导入
```

sqoop2使用方式

1)connector

```
generic-jdbc-connector
hdfs-connector
kafka-connector
kite-connector
```

2)link
3)job

cdc增量数据收集

canal阿里巴巴旗下利用Java开发的一款数据库增量日志解析系统

otter阿里巴巴给予canal开发的一款解决多机房同步数据问题的系统

------

### 非关系型数据的收集

1）flume设计动机
日志

数据源种类繁多

数据源是物理分布的

流式的，不间断产生的

对可靠性有一定要求

2）flume基本思想及特点

良好的扩展性

高度定制化

声明式动态化配置

语意路由

flume NG

基本架构
flume传递的数据称之为event

source
channel

flume NG高级组件

interceptor

sink processor

flume NG数据流拓扑构建方法
1）如何构建数据流拓扑
流式数据获取方式
远程过程调用
tcp或是udp
执行命令

常见的拓扑结构

多路合并和多路复用

agent配置方式

数据流拓扑实例剖析

### 分布式消息队列kafka

kafka设计动机

每一条业务都是一条数据流水线

数据生产者和消费者耦合度过高，当需要增加一种新的消费者时，所有数据生产者均需要被改动
使得整个数据流水线扩展性非常差

生产者和消费者数据处理速率不对等

大量并发的网络连接对后端消费者不够友好

消息中间件

消息队列

kafka特点

高性能

良好对扩展性

数据持久性

kafka设计架构

producer

broker

consumer

kafka关键技术点

```
可控的可靠性级别
数据多副本
高效的持久化机制
数据传输优化，批处理与zero-copy技术
可控的消息传递语义
```

kafka程序设计

producer程序设计

kafka典型应用场景

```
消息队列
流式计算框架的数据源
分布式日志收集系统中的source或sink
lambda architecture中的source
```

### 数据存储

#### 数据序列化与文件存储格式

```
数据序列化
文件存储格式
存储系统
```

#### 数据序列化的意义

阶段1:不考虑任何复杂的序列化方案，直接将数据转化成字符串，以文本形式保存或传输

但是难以嵌套数据，无法表达二进制数据，难以应对数据模式变化

阶段2:采用编程语言的序列化机制，比如Java serialization，python pickle

很难做到跨语言数据的写入和输出

阶段3:json和xml

严重的性能问题，解析速度慢，数据存储冗余较大，比如json会重复存储每个属性的名称

阶段4:期望出现一种带有schema描述的数据表示格式，通过统一化的schema描述，可约束每个字段的类型，进而为存储和解析数据带来优化的可能，此外，统一schema的引入，可减少属性名称重复存储带来的开销，同时，也有利于数据共享

序列化框架
thrift，protocol，buffers和avro

idl接口描述语言

#### 数据序列化方案

序列化框架thrift

序列化框架protobuf

序列化框架avro

序列化框架对比

性能方面

非功能方面

文件存储格式解析

行存储与列存储

行存储



列存储

orc文件

支持复杂数据类型



## 大数据应用



### 企业图谱





### 智能推荐

### 推荐系统

#### 基于人口统计学的推荐

基于人口统计学的推荐机制（Demographic-based Recommendation）是一种最易于实现的推荐方法，它只是简单的根据系统用户的基本信息发现用户的相关程度，然后将相似用户喜爱的其他物品推荐给当前用户

#### 基于内容的推荐

基于内容的推荐是在推荐引擎出现之初应用最为广泛的推荐机制，它的核心思想是根据推荐物品或内容的元数据，发现物品或者内容的相关性，然后基于用户以往的喜好记录，推荐给用户相似的物品。

#### 基于协同过滤的推荐

随着 Web2.0 的发展，Web 站点更加提倡用户参与和用户贡献，因此基于协同过滤的推荐机制因运而生。它的原理很简单，就是根据用户对物品或者信息的偏好，发现物品或者内容本身的相关性，或者是发现用户的相关性，然后再基于这些关联性进行推荐。基于协同过滤的推荐可以分为三个子类：基于用户的推荐（User-based Recommendation），基于项目的推荐（Item-based Recommendation）和基于模型的推荐（Model-based Recommendation）

- 基于用户的协同过滤推荐

基于用户的协同过滤推荐的基本原理是，根据所有用户对物品或者信息的偏好，发现与当前用户口味和偏好相似的“邻居”用户群，在一般的应用中是采用计算“K- 邻居”的算法；然后，基于这 K 个邻居的历史偏好信息，为当前用户进行推荐

- 基于项目的协同过滤推荐

基于项目的协同过滤推荐的基本原理也是类似的，只是说它使用所有用户对物品或者信息的偏好，发现物品和物品之间的相似度，然后根据用户的历史偏好信息，将类似的物品推荐给用户

- 基于模型的协同过滤推荐

基于模型的协同过滤推荐就是基于样本的用户喜好信息，训练一个推荐模型，然后根据实时的用户喜好的信息进行预测，计算推荐

#### 混合推荐

 在现行的 Web 站点上的推荐往往都不是单纯只采用了某一种推荐的机制和策略，他们往往是将多个方法混合在一起，从而达到更好的推荐效果。关于如何组合各个推荐机制，这里讲几种比较流行的组合方法。

```
加权的混合（Weighted Hybridization）: 用线性公式（linear formula）将几种不同的推荐按照一定权重组合起来，具体权重的值需要在测试数据集上反复实验，从而达到最好的推荐效果。
切换的混合（Switching Hybridization）：前面也讲到，其实对于不同的情况（数据量，系统运行状况，用户和物品的数目等），推荐策略可能有很大的不同，那么切换的混合方式，就是允许在不同的情况下，选择最为合适的推荐机制计算推荐。
分区的混合（Mixed Hybridization）：采用多种推荐机制，并将不同的推荐结果分不同的区显示给用户。其实，Amazon，当当网等很多电子商务网站都是采用这样的方式，用户可以得到很全面的推荐，也更容易找到他们想要的东西。
分层的混合（Meta-Level Hybridization）: 采用多种推荐机制，并将一个推荐机制的结果作为另一个的输入，从而综合各个推荐机制的优缺点，得到更加准确的推荐。
```

