# 数据科学

## 数理基础
## 数据库之sql


> https://www.jianshu.com/p/9e91aa8782da

### mysql



### oracle



umber 数据类型

number (precision,scale)

a)    precision表示数字中的有效位，如果没有指定precision的话，oracle将使用38作为精度；

b)    如果scale大于零，表示数字精度到小数点右边的位数；scale默认设置为0；如果scale小于零，oracle将把该数字取舍到小数点左边的指定位数。

c)    Precision 的取值范围是[1-38];scale的取值范围是[-84-127].

d)    Number整数部分允许的长度为(precision –scale),无论scale是正数还是负数。

e)    如果precision小于scale，表示存储的是没有正数的小数。

f)     Precision表示有效位数，有效数位：从左边第一个不为0的数算起，小数点和负号不计入有效位数；scale表示精确到多少位，指精确到小数点左边还是右边多少位（由+-决定）。

a)    关于precision，scale也可以做如下表述：

定点数的精度(p)和刻度(s)遵循以下规则：

1） 当一个数的整数部分长度 >p-s时，oracle就会报错；

2） 当一个数的小数部分的长度 >s时，oracle就会舍入；

3） 当s(scale)为负数时，oracle就会对小数点左边的s进行舍入；

4） 当s > p 时, p表示小数点后第s位向左最多可以有多少位数字，如果大于p则Oracle报错，小数点后s位向右的数字被舍入。

Number类型的子类：

a)    Oracle本来就没有int类型，为了与别的数据库兼容，新增了Int类型作为number类型的子集；

b)    Int类型只能存储整数，number可以存储浮点数，也可以存整数。

c)    在oracle数据库建表的时候，decimal，numeric不带精度，oralce会自动把它处理成integer；带精度，oracle会自动把它处理成number。

d)    Oracle只用number(m,n)就可以表示任何复杂的数字数据。

Decimal，numeric，int等都为sql,db2等数据库的数据类型，Oracle为了兼容才将其引入；但实际上在oracle内部还是以number的形式将其存入的。s

#### 参考链接

> https://wiki.centos.org/zh/HowTos?highlight=%28%28zh%7CHowTos%7COracle12onCentos7%29%29

### mongodb



### redis

### PostgreSQL

> https://www.postgresql.org/docs/11/tutorial-createdb.html

#### PostgreSQL

`PostgreSQL`是以加州大学伯克利分校计算机系开发的 POSTGRES，现在已经更名为PostgreSQL，版本 4.2为基础的**对象关系型数据库管理系统**（ORDBMS）。PostgreSQL支持大部分 SQL标准并且提供了许多其他现代特性：复杂查询、外键、触发器、视图、事务完整性、MVCC。同样，PostgreSQL 可以用许多方法扩展，比如， 通过增加新的数据类型、函数、操作符、聚集函数、索引。免费使用、修改、和分发 PostgreSQL，不管是私用、商用、还是学术研究使用。

##### psql

是PostgreSQL数据库的命令行交互工具。

##### pgAdmin

是PostgreSQL数据库的图形化管理工具。





### PostgreSQL安装与卸载

#### 官网安装

在官网上下载安装包或者使用官网提供的`Postgres.app`，这里就不详细介绍了，因为我们这里使用`homebrew`来安装。

#### homebrew安装

```
brew install postgresql
```

初始化：

```
initdb /usr/local/var/postgres
```

### 创建数据库和账户

#### 创建数据库和账户

mac安装PostgreSQL后不会创建用户名数据库，执行命令：

```
createdb
```

然后登录PostgreSQL控制台：

```
psql
```

`psql`连接数据库默认选用的是当前的系统用户

使用\l命令列出所有的数据库，看到已存在用户同名数据库、postgres数据库，但是postgres数据库的所有者是当前用户，没有postgres用户。

然后我们来完成以下几件事：

一、创建postgres用户

```
CREATE USER postgres WITH PASSWORD 'XXXXXX';
```

二、删除默认生成的postgres数据库

```
DROP DATABASE postgres;
```

三、创建属于postgres用户的postgres数据库

```
CREATE DATABASE postgres OWNER postgres;
```

四、将数据库所有权限赋予postgres用户

```
GRANT ALL PRIVILEGES ON DATABASE postgres to postgres;
```

五、给postgres用户添加创建数据库的属性

```
ALTER ROLE postgres CREATEDB;
```

这样就可以使用postgres作为数据库的登录用户了，并可以使用该用户管理数据库

##### 登陆控制台指令

```
psql -U [user] -d [database] -h [host] -p [port]
```

-U指定用户，-d指定数据库，-h指定服务器，-p指定端口
完整的登录命令，比如使用postgres用户登录

```
psql -U postgres -d postgres
```

之前我们直接使用psql登录控制台，实际上使用的是缺省数据

```
user：当前mac用户
database：用户同名数据库
主机：localhost
端口号：5432，postgresql的默认端口是5432
```

##### 常用控制台指令

```
\password：设置当前登录用户的密码
\h：查看SQL命令的解释，比如\h select。
\?：查看psql命令列表。
\l：列出所有数据库。
\c [database_name]：连接其他数据库。
\d：列出当前数据库的所有表格。
\d [table_name]：列出某一张表格的结构。
\du：列出所有用户。
\e：打开文本编辑器。
\conninfo：列出当前数据库和连接的信息。
\password [user]: 修改用户密码
\q：退出
```

##### 使用PostgreSQL

现在来简单的学习一下使用PostgreSQL，以下命令都在postgres=# 环境下
修改用户密码
之前我们用命令CREATE USER postgres WITH PASSWORD 'XXXXXX';创建了postgres用户，现在我们来修改该用户的密码：

```
ALTER USER postgres WITH PASSWORD 'XXXXXX'
```

出现ALTER ROLE, 代表修改角色成功

#### 创建和删除数据库用户

创建user1用户：`CREATE USER user1 WITH PASSWORD 'XXXX'`

查看数据库用户列表：`\du`

删除数据库用户：`drop user user1;`

#### 创建和删除数据库

创建数据库：`create database testdb;`

查看数据库列表：`\l`

删除数据库：`drop database db1;`

#### 创建和删除数据表

选择数据库：`\c DatabaseName`，比如`\c testdb`

创建数据库表：`CREATE TABLE COMPANY( ID INT PRIMARY KEY NOT NULL, NAME TEXT NOT NULL, AGE INT NOT NULL, ADDRESS CHAR(50), SALARY REAL);`

删除数据库表： `drop table company;`

查看数据库信息：`\d`

查询数据：`select * from company`

#### 关于第三方连接本地数据库的问题

如何设置PostgreSQL允许被远程访问

```
/usr/local/var/postgres/postgresql.conf
```

1. 修改postgresql.conf
   编辑或添加下面一行，使PostgreSQL可以接受来自任意IP的连接请求。

```
listen_addresses = '*'
```

1. 修改pg_hba.conf
   pg_hba.conf，位置与postgresql.conf相同，虽然上面配置允许任意地址连接PostgreSQL，但是这在pg中还不够，我们还需在pg_hba.conf中配置服务端允许的认证方式。任意编辑器打开该文件，编辑或添加下面一行。

```
# TYPE  DATABASE  USER  CIDR-ADDRESS  METHOD
host  all  all 0.0.0.0/0 md5
```

默认pg只允许本机通过密码认证登录，修改为上面内容后即可以对任意IP访问进行密码验证。对照上面的注释可以很容易搞明白每列的含义，具体的支持项可以查阅文末参考引用。

完成上两项配置后执行`sudo service postgresql restart`重启PostgreSQL服务后，允许外网访问的配置就算生效了。





## 利用python进行数据分析

> 利用python进行数据分析，包含了基本的数据清洗、聚合、可视化部分的内容，需要多学习几遍



### python读取文件



```python
# coding:utf-8


file = open('test.txt')
while True:
    line = file.readline()
    if not line:
        break
    print(line, end='')


file = open('test.txt', 'rb')
while True:
    chunk = file.read(10)
    if not chunk:
        break
    print(chunk, end='')
```




## excel


## 特征处理

## 机器学习



### bagging和boosting的区别

　　Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。

首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。

1、Bagging (bootstrap aggregating)

Bagging即套袋法，其算法过程如下：

A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

 

2、Boosting

其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

关于Boosting的两个核心问题：

1）在每一轮如何改变训练数据的权值或概率分布？

通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

2）通过什么方式来组合弱分类器？

通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。

而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

 

3、Bagging，Boosting二者之间的区别

Bagging和Boosting的区别：

1）样本选择上：

Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

2）样例权重：

Bagging：使用均匀取样，每个样例的权重相等

Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

3）预测函数：

Bagging：所有预测函数的权重相等。

Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

4）并行计算：

Bagging：各个预测函数可以并行生成

Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。

 

4、总结

这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。

下面是将决策树与这些算法框架进行结合所得到的新的算法：

1）Bagging + 决策树 = 随机森林

2）AdaBoost + 决策树 = 提升树

3）Gradient Boosting + 决策树 = GBDT





### 集成学习法

> https://www.cnblogs.com/zongfa/p/9304353.html

弱分类器组成强分类器

在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。

集成方法是将几种机器学习技术组合成一个预测模型的元算法，以达到减小方差（bagging）、偏差（boosting）或改进预测（stacking）的效果。

集成学习在各个规模的数据集上都有很好的策略。

数据集大：划分成多个小数据集，学习多个模型进行组合

数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合

 

集合方法可分为两类：

 

```
序列集成方法，其中参与训练的基础学习器按照顺序生成（例如 AdaBoost）。序列方法的原理是利用基础学习器之间的依赖关系。通过对之前训练中错误标记的样本赋值较高的权重，可以提高整体的预测效果。
并行集成方法，其中参与训练的基础学习器并行生成（例如 Random Forest）。并行方法的原理是利用基础学习器之间的独立性，通过平均可以显著降低错误。
```

 



总结一下，集成学习法的特点：

　　①  将多个分类方法聚集在一起，以提高分类的准确率。

（这些算法可以是不同的算法，也可以是相同的算法。）

　　②  集成学习法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行投票来进行分类

　　③  严格来说，集成学习并不算是一种分类器，而是一种分类器结合的方法。

　　④  通常一个集成分类器的分类性能会好于单个分类器

　　⑤  如果把单个分类器比作一个决策者的话，集成学习的方法就相当于多个决策者共同进行一项决策。

自然地，就产生两个问题：

　　1）怎么训练每个算法？

　　2）怎么融合每个算法？

这篇博客介绍一下集成学习的几个方法：Bagging，Boosting以及Stacking。

1、Bagging（bootstrap aggregating，装袋）

　　Bagging即套袋法，先说一下bootstrap，bootstrap也称为自助法，它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间，其算法过程如下：

　　A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）

　　B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）

　　C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）

由此，总结一下bagging方法：

　　①  Bagging通过降低基分类器的方差，改善了泛化误差
　　②  其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起
　　③  由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例

　　常用的集成算法类是随机森林。

　　在随机森林中，集成中的每棵树都是由从训练集中抽取的样本（即 bootstrap 样本）构建的。另外，与使用所有特征不同，这里随机选择特征子集，从而进一步达到对树的随机化目的。

因此，随机森林产生的偏差略有增加，但是由于对相关性较小的树计算平均值，估计方差减小了，导致模型的整体效果更好。

2、Boosting

其主要思想是将弱分类器组装成一个强分类器。在PAC（probably approximately correct，概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。

关于Boosting的两个核心问题：

　　1）在每一轮如何改变训练数据的权值或概率分布？

　　通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。

　　2）通过什么方式来组合弱分类器？

　　通过加法模型将弱分类器进行线性组合，比如：

　　AdaBoost（Adaptive boosting）算法：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。

　　GBDT（Gradient Boost Decision Tree)，每一次的计算是为了减少上一次的残差，GBDT在残差减少（负梯度）的方向上建立一个新的模型。





3、Stacking

　　Stacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。

　　如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，然后将输出用于训练第二层分类器。



二、Bagging，Boosting二者之间的区别

　　1、Bagging和Boosting的区别：

　　1）样本选择上：

　　Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。

　　Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。

　　2）样例权重：

　　Bagging：使用均匀取样，每个样例的权重相等

　　Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。

　　3）预测函数： 

　　Bagging：所有预测函数的权重相等。

　　Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。

　　4）并行计算： 

　　Bagging：各个预测函数可以并行生成

　　Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。　

　    2、决策树与这些算法框架进行结合所得到的新的算法：

　　1）Bagging + 决策树 = 随机森林

　　2）AdaBoost + 决策树 = 提升树

　　3）Gradient Boosting + 决策树 = GBDT

参考博文：

【1】集成学习总结 & Stacking方法详解  https://blog.csdn.net/willduan1/article/details/73618677

【2】Bagging和Boosting 概念及区别  https://www.cnblogs.com/liuwu265/p/4690486.html

【3】集成学习法之bagging方法和boosting方法 https://blog.csdn.net/qq_30189255/article/details/51532442

【4】机器学习中的集成学习（Ensemble Learning)  http://baijiahao.baidu.com/s?id=1590266955499942419&wfr=spider&for=pc

【5】简单易学的机器学习算法——集成方法(Ensemble Method) https://blog.csdn.net/google19890102/article/details/46507387




## 深度学习


## 可视化

## DW/BI



## 数据仓库

### 生命周期导论



#### 生命周期的历史

方法总结：业务维生命周期（business dimension lifecycle）。能否成功使用数据仓库技术管理数据取决于以下三个基本原则：

- 将关注点放在业务层。
- 按照维度组织数据，将这些数据以即席查询或数据报表的形式提交给业务用户。
- 在开发整个数据仓库的过程中，应当采用逐次迭代的方法，每次的生命周期增量都应当是可处理的，不要试图一次性完成所有的工作。

引入事实表、维度表和缓慢维度变换等概念之后，正式采用'kimball 生命周期'作为这一方法的统称。'技术源于实践，结论来自验证'





### 星型模型和雪花模型

#### 一、概述

　　在多维分析的商业智能解决方案中，**根据事实表和维度表的关系，又可将常见的模型分为星型模型和雪花型模型。**在设计逻辑型数据的模型的时候，就应考虑数据是按照星型模型还是雪花型模型进行组织。

　　当所有维表都**直接**连接到“ 事实表”上时，整个图解就像星星一样，故将该模型称为星型模型，

　　

　　

　　**星型架构是一种非正规化的结构，多维数据集的每一个维度都直接与事实表相连接，不存在渐变维度，所以数据有一定的冗余**，

　　如在地域维度表中，存在国家 A 省 B 的城市 C 以及国家 A 省 B 的城市 D 两条记录，那么国家 A 和省 B 的信息分别存储了两次，即存在冗余。

　　当有**一个或多个维表没有直接连接到事实表上**，而是通过其他维表连接到事实表上时，其图解就像多个雪花连接在一起，故称雪花模型。

　　

 

　　雪花模型是对星型模型的扩展。它对星型模型的维表进一步层次化，原有的各维表可能被扩展为小的事实表，形成一些局部的 " 层次 " 区域，这些被分解的表都连接到主维度表而不是事实表。如图 2，将地域维表又分解为国家，省份，城市等维表。

　　它的优点是 : **通过最大限度地减少数据存储量以及联合较小的维表来改善查询性能。雪花型结构去除了数据冗余。**

　　**此在冗余可以接受的前提下，实际运用中星型模型使用更多，也更有效率（空间换易用与效率）。**

#### **二、使用选择**

　　**1.数据优化**

　　雪花模型使用的是规范化数据，也就是说数据在数据库内部是组织好的，以便消除冗余，因此它能够有效地减少数据量。通过引用完整性，其业务层级和维度都将存储在数据模型之中。

▲图1 雪花模型

　　相比较而言，星形模型实用的是反规范化数据。在星形模型中，维度直接指的是事实表，业务层级不会通过维度之间的参照完整性来部署。

▲图2 星形模型

 

　　**2.业务模型**

　　主键是一个单独的唯一键(数据属性)，为特殊数据所选择。在上面的例子中，Advertiser_ID就将是一个主键。外键(参考属性)仅仅是一个表中的字段，用来匹配其他维度表中的主键。在我们所引用的例子中，Advertiser_ID将是Account_dimension的一个外键。

　　在雪花模型中，数据模型的业务层级是由一个不同维度表主键-外键的关系来代表的。而在星形模型中，所有必要的维度表在事实表中都只拥有外键。

　　**3.性能**

　　第三个区别在于性能的不同。雪花模型在维度表、事实表之间的连接很多，因此性能方面会比较低。举个例子，如果你想要知道Advertiser 的详细信息，雪花模型就会请求许多信息，比如Advertiser  Name、ID以及那些广告主和客户表的地址需要连接起来，然后再与事实表连接。

而星形模型的连接就少的多，在这个模型中，如果你需要上述信息，你只要将Advertiser的维度表和事实表连接即可。

　　**4.ETL**

　　雪花模型加载数据集市，因此ETL操作在设计上更加复杂，而且由于附属模型的限制，不能并行化。

　　星形模型加载维度表，不需要再维度之间添加附属模型，因此ETL就相对简单，而且可以实现高度的并行化。

　　**总结**

　　通过上面的对比，我们可以发现数据仓库大多数时候是比较适合使用星型模型构建底层数据Hive表，通过大量的冗余来提升查询效率，星型模型对OLAP的分析引擎支持比较友好，这一点在Kylin中比较能体现。而雪花模型在关系型数据库中如MySQL，Oracle中非常常见，尤其像电商的数据库表。在数据仓库中雪花模型的应用场景比较少，但也不是没有，所以在具体设计的时候，可以考虑是不是能结合两者的优点参与设计，以此达到设计的最优化目的。

　　**参考链接**：<http://blog.csdn.net/u010454030/article/details/74589791>

#### 三、建模四步走

　　**1.选取要建模的业务处理流程**

　　　　关注业务处理流程，而不是业务部门！

　　**2.定义业务处理的粒度**

　　　　“如何描述事实表的单个行？”

　　**3.选定用于每个事实表行的维度**

　　　　常见维度包括日期、产品等

　　**4.确定用于形成每个事实表行的数字型事实**

　　　　典型的事实包括订货量、支出额这样的可加性数据

　　　　对于通过计算而得到的事实是否应该物理地存放在数据库表中，《工具箱》中给出的建议是应该存放，利用少量的存储空间来避免用户的计算错误，是可取的，但是对于不可加的数据（例如零售中的毛利润率、单价等）不用单独存放，只需存放销量与销售额、毛利润这样的在各个纬度上的可加性数据即可。对于利率这样的百分比数据，事实表中存放分子、分母即可！

 

 

 　　小结就是：确定业务流程->确定粒度->确定纬度->确定事实





## DataTalk：是一个宽表好还是多个维表好？



### 0x00 前言

本篇的主题是关于数据模型的规范化和反规范化的讨论，其实也是一种常见的维度建模的设计和业务使用便捷性的冲突。

### 0x01 讨论

**问题：**

在设计数据表的时候，是一个宽表好，还是多个维度表好？

**回答一：**

数据仓库每张表的搭建，主要依赖于这个表在整个数据仓库中的作用和相关意义。首先要清楚这个表的存在是为了解决那些问题，什么角色使用，怎么保证使用者尽可能好的体验解决问题。从以上所提到的角度去看待问题，拆解以下几点因素：

1. 拆表情况下多张数据表的查询SQL的编写难度有多大，是否会出现为了数据提取需要关联多张表，并且需要提前知道各个表之间的关联关系。如果使用这个数据的人员较多，每个人都需要先了解所需要多张表的关联关系，然后才进行数据查询，这样是不是维度沟通成本较高，查询体验下降，影响使用者的工作效率？
2. 多表关联查询的使用频次有多高，将重复高频的事情简化，是不是更好？
3. 查询体验上需要考虑多表关联之后的查询性能问题，如果一张表的内容过度，是否影响查询速度？
4. 多表关联的合理性，不同的数据维度和内容与订单表关联，是不是会存在违背常理的坑存在。比如，数据字段的对应关系是一对一，还是多对多，是否会让使用者忽略查询数据时候的过滤限制条件。
5. 数据的安全问题，每张数据表的安全范围不同，合并成同一张表是面临的是更大的权限开放。比如订单表可能仅需要让一部分人员知晓订单信息，并不想让他们知道供应商信息。

**回答二：**

结合我司的一些经验来说说哈，我司会将数据用于各种各样不同主题和纬度的报表,也会将数据用于数据挖掘做模型的，因此数据分成肯定是必要的，针对报表类的数据根据报表的不同反向划分出不同的纬度表,这种方式其实就是将mysql业务库的数据经过sql语句之后重新生成一张或者多张维度表,在这之中根据经验会抽取出一个经常用的字段作为公共字段放入公共层数据中,一些经常需要用到的度量值也会抽取到度量表中,那么一些非开发人员来看数据的时候只要在页面上简单写几个sql语句就可以统计出数据来,比如月销量,周销量,日销量这些。

若是机器学习模型的同学要数据的话,我们就只需要从维度表,度量表,事实表中抽取数据做成大宽表给他们了,由于模型做的比较少,对于大宽表的经验比较少,暂时只能来一个模型数据的需求,单独写sql语句去抽取。

### 0x02 补充

这个问题，从本质上来讲。想讨论是数据模型设计里面的规范化和反规范化的问题。

从规范化的角度来讲，数据仓库的设计者是希望越规范越好，因为这样会减少数据的冗余，而且也便于模型的扩展。从反规范化的角度来讲，数据仓库的使用者是希望使用越方便越好，他们并不太关系规范不规范冗余不冗余，只要用着方便就好。

这种情况在工作中是十分常见的，那么该怎样来解决它？下面有两个思路：

1. 两种方式都存。虽然，这样看起来会占用更多的存储空间，但不失为一种合适的解决方案，因为宽表是通过别的表拼接而成的，因此宽表的存储周期是可以短一些。
2. 只存多个维度表，通过视图来创建宽表。这种方式适合于宽表的查询次数较少的情况。比如在Hive中，宽表其实只是为了计算出来之后导入Es等系统中供其它系统查询，那么久没必要存储一份宽表，直接通过视图来封装就可以。

另外，数据仓库的设计，往往不能是以计算出几张表就结束了，我们更应该提供的是数据服务，让使用方都通过服务的方式来访问我们的数据，而不是简单地将表暴露出去。当我们以数据服务的方式提供数据的时候，不管是易用性还是安全性都更容易得到满足。

### 0xFF 总结

感谢 Joker 和 Alan 的回答，感谢 Rebie 的整理，感谢木东居士的总结（自己感谢自己，_）。

DataTalk 系列的文章结构一般分为三部分：

- 第一部分是居士的一个小的前言，大致明确该篇的主题
- 第二部分是问题讨论的主体部分，居士会对大家讨论的内容进行总结和梳理，尽量保证原汁原味。
- 第三部分是居士的总结，主观性比较强，算是自己的理解。

一般来讲，每一篇文章都会对应到 GitHub 中的一个 Issue，比如本篇讨论内容的地址为：[https://github.com/dantezhao/data-group/issues/1](https://link.jianshu.com?t=https%3A%2F%2Fgithub.com%2Fdantezhao%2Fdata-group%2Fissues%2F1)

------

## 维度建模



遵循这些原则进行维度建模可以保证数据粒度合理，模型灵活，能够适应未来的信息资源，违反这些原则你将会把用户弄糊涂，并且会遇到数据仓库障碍。

**原则一**: 载入详细的原子数据到维度结构中

> 维度建模应该使用最基础的**原子数据**进行填充，以支持不可预知的来自用户查询的过滤和分组请求，用户通常不希望每次只看到一个单一的记录，但是你无法预测用户想要掩盖哪些数据，想要显示哪些数据，如果只有汇总数据，那么你已经设定了数据的使用模式，当用户想要深入挖掘数据时他们就会遇到障碍。当然，原子数据也可以通过概要维度建模进行补充，但企业用户无法只在汇总数据上工作，他们需要原始数据回答不断变化的问题。

**原则二**: 围绕业务流程构建维度模型

> 业务流程是组织执行的活动，它们代表可测量的事件，如下一个订单或做一次结算，业务流程通常会捕获或生成唯一的与某个事件相关的性能指标，这些数据转换成事实后，每个**业务流程**都用一个原子事实表表示，除了单个流程事实表外，有时会从多个流程事实表合并成一个事实表，而且合并事实表是对单一流程事实表的一个很好的补充，并不能代替它们。　　

**原则三**: 确保每个事实表都有一个与之关联的日期维度表　　

> 原则二中描述的可测量事件总有一个日期戳信息，每个事实表至少都有一个外键，关联到一个日期维度表，它的粒度就是一天，使用日历属性和非标准的关于测量事件日期的特性，如财务月和公司假日指示符，有时一个事实表中有多个日期外键。　　

**原则四**: 确保每个事实表中的事实具有相同的粒度或同级的详细程度　　

> 在组织事实表时粒度上有三个基本原则：**事务，周期快照或累加快照**。无论粒度类型如何，事实表中的度量单位都必须达到相同水平的详细程度，如果事实表中的事实表现的粒度不一样，企业用户会被搞晕，BI应用程序会很脆弱，或者返回的结果根本就不对。　

**原则五**: 解决事实表中的多对多关系　　

> 由于事实表存储的是业务流程事件的结果，因此在它们的外键之间存在多对多(M:M  )的关系，如多个仓库中的多个产品在多天销售，这些外键字段不能为空，有时一个维度可以为单个测量事件赋予多个值，如一个保健对应多个诊断，或多个客户有一个银行账号，在这些情况下，它的不合理直接解决了事实表中多值维度，这可能违反了测量事件的天然粒度，因此我们使用多对多，双键桥接表连接事实表。　　

**原则六**: 解决维度表中多对一的关系　　

> 属性之间分层的: 多对一(M：1)的关系通常未规范化，或者被收缩到扁平型维度表中，如果你曾经有过为事务型系统设计实体关系模型的经历，那你一定要抵抗住旧有的思维模式，要将其规范化或将M:1关系拆分成更小的**子维度**，**维度反向规范化**是维度建模中常用的词汇。在单个维度表中多对一(M:1)的关系非常常见，一对一的关系，如一个产品描述对应一个产品代码，也可以在维度表中处理，在事实表中偶尔也有多对一关系，如详细当维度表中有上百万条记录时，它推出的属性又经常发生变化。不管怎样，在事实表中要慎用M:1关系。　　

**原则七**: 存储报告标记和过滤维度表中的范围值　　

> 更重要的是，编码和关联的解码及用于标记和查询过滤的描述符应该被捕获到维度表中，避免在事实表中存储神秘的编码字段或庞大的描述符字段，同样，不要只在维度表中存储编码，假定用户不需要描述性的解码，或它们将在BI应用程序中得到解决。如果它是一个行/列标记或下拉菜单过滤器，那么它应该当作一个维度属性处理。尽管我们在原则5中已经陈述过，事实表外键不应该为空，同时在维度表的属性字段中使用“NA”或另一个默认值替换空值来避免空值也是明智的，这样可以减少用户的困惑。

**原则八**: 确定维度表使用了代理键　　

> 按顺序分配代理键(除了日期维度)可以获得一系列的操作优势，包括更小的事实表:  索引以及性能改善，如果你正在跟踪维度属性的变化，为每个变化使用一个新的维度记录，那么确实需要代理键，即使你的商业用户没有初始化跟踪属性改变的设想值，使用代理也会使下游策略变化更宽松，代理也允许你使用多个业务键映射到一个普通的配置文件，有利于你缓冲意想不到的业务活动，如废弃产品编号的回收或收购另一家公司的编码方案。　　

**原则九**: 创建一致的维度集成整个企业的数据　　

> 对于企业数据仓库一致的维度(也叫做通用维度:  标准或参考维度)是最基本的原则，在ETL系统中管理一次，然后在所有事实表中都可以重用，一致的维度在整个维度模型中可以获得一致的描述属性，可以支持从多个业务流程中整合数据，企业数据仓库总线矩阵是最关键的架构蓝图，它展现了组织的核心业务流程和关联的维度，重用一致的维度可以缩短产品的上市时间，也消除了冗余设计和开发过程，但一致的维度需要在数据管理和治理方面有较大的投入。　　

**原则十**: 不断平衡需求和现实，提供用户可接受的并能够支持他们决策的DW/BI解决方案　　

> 维度建模需要不断在用户需求和数据源事实之间进行平衡，才能够提交可执行性好的设计，更重要的是，要符合业务的需要，需求和事实之间的平衡是DW/BI从业人员必须面对的事实，无论是你集中在维度建模，还是项目策略:  技术/ETL/BI架构或开发/维护规划都要面对这一事实

## 数据中心建设之路

### 集成和独立，该选择哪个？

​	对DW/BI整体环境的构建进行规划是一项极为重要的活动，从历史的角度来看，争议的热点主要在于应该从建立一个集中式的、规划好的架构的角度为整个企业建立数据仓库，还是为每个具体的业务单位建立小型的独立解决方案。当然，这两种方式都不怎么有效。

​	建立集中式大型数据仓库的方法需要很长一段时间的开发工作才能体现出其业务价值，因此业务用户常常会对其失去兴趣，并且最终使开发过程陷入步履艰难的境地。而另一方面，尽管建立独立的部门系统见效很快，但是由于这种方法不断地增加数据烟囱，因此很快也会出现问题。

```
面向部门的独立解决方案常常被称作数据集市，当我们在20世纪90年代使用这一术语时，我们使用它描述以过程为中心的复杂数据库，它描述的是企业整体数据架构的一个子集，和那种独立的解决方案大不相同，后来这个术语被抢去指代独立的，非体系化的部门数据库，鉴于对数据集市这一术语的定义差异很大，而且理解也不一致，我们将数据集市这一术语去掉了。
```

​	设计企业DW/BI数据架构的任务令人望而生畏，大型企业中刚任命的DW/BI项目群经理往往面临两个巨大的、看起来并不相关的挑战。一方面，项目经理必须去了解企业最复杂的资产-源数据的有关内容，包括一些遗留框架、ERP、web服务器、应用服务器和其他工作系统的内容。必须知道所有数据元素，并对数据元素进行清洗和更正。

​	但是我们必须要建设一个框架将那些独立的系统集成起来形成一个耦合的整体。无法有效地将孤立的、单独的数据烟囱式的解决方案集成到一起已经成为DW/BI系统的致命伤，这比错失进行数据分析的时机要严重得多。独立的烟囱系统是没有出路的，只能继续使用和企业不兼容的视图。烟囱式的解决方案给出的报表无法相互进行比较，正是其自身的原因使他们变成了遗留系统。烟囱式解决方案的存在严重阻碍了集成式的企业数据仓库的开发进程。

### 如果一次性创建数据仓库过于困难，需要将整个目标划分为若干独立小块予以实现，那么应该做哪些工作？

​	要解决这样一个两难问题，首先要迅速而简洁地定义整个企业DW/BI系统的数据架构。在初期的项目群层需求收集过程中，最终生成一个企业数据仓库总线矩阵。矩阵的每一行都对应机构中的一个业务过程，每一列都和一个业务维度相对应。

![数据仓库总线](pages_html/s02_%E6%95%B0%E6%8D%AE%E6%99%BA%E8%83%BD/images/%E6%95%B0%E6%8D%AE%E4%BB%93%E5%BA%93%E6%80%BB%E7%BA%BF.jpg)

​	企业数据仓库总线矩阵是DW/BI系统的一个总体数据架构。矩阵给出了系统全貌的透视图，并不考虑数据库和技术参数，但是合理地识别了易于管理的开发工作。每个业务过程的实现都对整个架构进行了增量扩展。

​	通过总线矩阵开发数据架构是一种理性的方法，可以将企业数据仓库设计过程中那些令人生畏的工作分解开来。总线矩阵确立了一个架构框架，指导总体设计，在此基础上可以将整个问题分割为很多便于实现的小块。应该迭代地建立一个集成的企业数据仓库，一个业务过程一个业务过程地进行，并使用一组共享的一致性维度来确保实现系统的综合集成。这些一致性维度在整个企业范围内有统一的解释。最后，整个企业数据仓库就呈现出：一个强大的基于一致性维度的架构将一组业务过程紧密联系到一起就形成了企业数据仓库。

​	一致性维度是企业数据仓库的总线，通过为DW/BI环境提供一个标准的总线接口，可以将新的业务过程引入数据仓库，该业务过程从总线获得动力，并且和其他已经存在的业务过程和谐共存。

### 价值链的意义

​	很多业务都通过一系列业务过程步骤来监督内部活动逻辑流程。每个业务过程产生一个或多个事实表，因为业务过程中每个步骤都是一个唯一的度量事件。

​	在制造业中，一个产品的流动过程是从获取原材料到加工成成品，最终交付给客户的整个过程。从原始起点到消费终点的整个流程的管理常常被称作"供应链管理"。

### 通用矩阵常见问题

矩阵行问题

- 涉及整个部门的行和包含内容过多的行
- 以报表为中心的行和定义过窄的行

矩阵列问题

- 过于一般化的列
- 为层次结构图中的每一个层次都分配单独的列

### 坚持使用一致性维度

如果DW/BI团队成功为企业确立了一组主一致性维度，那么开发团队能否真正使用这些维度就成为非常重要的问题了。

### 维度深入

#### 日期和时间

在每个数据仓库中日期维都占有特殊的地位，这是因为几乎每个事实表都是一个观测值的时间序列，事实表总是有一个或者多个日期维。

必须将假期，工作日，结账期，作为标记的每月最后一天和其他分组或者过滤条件都植入维中。记住使用详细日期表的主要原因是为了将有关日程的所有信息彻底从BI应用程序中去除掉。日程导航应当通过日期维度表来驱动，而不是通过硬编码的应用程序逻辑来驱动。

- 代理日期键
- 时刻
- 日期/时间戳
- 多时区的情形

#### 退化维



#### 缓慢变化维

#### 杂项维

#### 雪花型和支架

#### 桥接表





## 爬虫

### 第一次爬虫



#### 爬虫示例-使用bs4

```python
# coding:utf-8
import requests
from bs4 import BeautifulSoup
from lxml import etree
from requests import RequestException
import json


def get_page_source(url):
    # 获取源码
    try:
        resp = requests.get(url)
        if resp.status_code == 200:
            resp.encoding = resp.apparent_encoding 
            return resp.text
        return ''
    except RequestException as e:
        print('error:', e)
        return ''


def parse_with_bs4(html_source):
    # 解析源码
    html=BeautifulSoup(html_source, 'lxml')
    # 创建css选择器
    items=html.select('script[type="text/javascript"]')
    for item in items:
        if "https://qr.alipay.com" in item.text:
            print(item)


def parse_with_xpath(html_source):
    # 解析源码
    s = etree.HTML(html_source)

    dictobj=s.xpath('//*[@id="J-barcode-container"]/canvas')

    with open("news.json", "a+", encoding='utf-8') as f:
        f.write(json.dumps(dictobj, ensure_ascii=False) + '\n')


def go():
    url = ''


if __name__ == "__main__":
    go()

```

### 爬虫的路线

对于爬虫，我个人的体会是，先从根网站开始解析。网站本身的建设结构就是树状的，里面隐含了很多内容。









## 探索性数据分析



### 主题定义

探索性数据分析（Exploratory Data Analysis，简称EDA），摘抄网上的一个中文解释，是指对已有的数据（特别是调查或观察得来的原始数据）在尽量少的先验假定下进行探索，通过作图、制表、方程拟合、计算特征量等手段探索数据的结构和规律的一种数据分析方法。特别是党我们对面对大数据时代到来的时候，各种杂乱的“脏数据”，往往不知所措，不知道从哪里开始了解目前拿到手上的数据时候，探索性数据分析就非常有效。探索性数据分析是上世纪六十年代提出，其方法有美国统计学家John Tukey提出的。



维基百科的英语解释：
In statistics, exploratory data analysis(EDA) is an approach to analyzing data sets to summarize their maincharacteristics, often with visual methods. A statistical model can be used ornot, but primarily EDA is for seeing what the data can tell us beyond theformal modeling or hypothesis testing task. Exploratory data analysis waspromoted by John Tukey to encourage statisticians to explore the data, andpossibly formulate hypotheses that could lead to new data collection andexperiments. EDA is different from initial data analysis (IDA), which focusesmore narrowly on checking assumptions required for model fitting and hypothesistesting, and handling missing values and making transformations of variables asneeded. EDA encompasses IDA.

百度翻译：

在统计学中，探索性数据分析（EDA）是一种分析数据集以概括其主要特征的方法，通常使用可视化方法。可以使用或使用统计模型，但主要是EDA是为了了解数据在形式化建模或假设测试任务之外能告诉我们什么。探索性数据分析是John Tukey提拔的鼓励统计学家的研究数据，并尽可能提出假设，尽可能生成新的数据收集和实验。EDA不同于初始数据分析（IDA），，它更集中于检查模型拟合和假设检验所需的假设，以及处理缺少的值，并根据需要进行变量转换。EDA包含IDA。



探索性分析的计划：
1、Form hypotheses/develop investigation theme to explore形成假设，确定主题去探索
2、Wrangle data清理数据，网上有一个网址公布斯坦福有一个软件叫datawrangler可以供大家自己免费下载，用于探索数据分析，很快的解决数据清洗的工作，作为一个将来想成为数据科学家的人，处理“脏数据”，是我们必须走的路。这个软件我还没有试，我把链接发在下面，供爱学习的小伙伴好好学习。http://vis.stanford.edu/wrangler/https://www.trifacta.com/products/wrangler/https://www.douban.com/note/501799325/
3、Assess quality of data评价数据质量
4、Profile data数据报表
5、Explore each individual variable in the dataset探索分析每个变量
6、Assess the relationship between each variable and the target探索每个自变量与因变量之间的关系
7、Assess interactions between variables探索每个自变量之间的相关性
8、Explore data across many dimensions从不同的维度来分析数据



通过以上的探索性分析，你还可以做以下的工作：1、写出一系列你自己做的假设，然后接着做更深入的数据分析2、记录下自己探索过程中更进一步的数据分析过程3、把自己的中间的结果给自己的同行看看，让他们能够给你一些更有拓展性的反馈、或者意见。不要独自一个人做，国外的思维就是知道了什么就喜欢open to everybody，要走出去，多多交流，打开新的世界。4、将可视化与结果结合一起。探索性数据分析，就是依赖你好的模型意识，（在《深入浅出数据分析》P34中，把模型的敏感度叫心智模型，最初的心智模型可能错了，一旦自己的结果违背自己的假设，就要立即回去详细的思考）。所以我们在数据探索的尽可能把自己的可视化图和结果放一起，这样便于进一步分析。



> 参考链接:https://www.jianshu.com/p/9325c9f88ee6



### 特征工程

### 数学建模

1.数学常用模型

#### 1.1运筹学模型

- 数学规划模型
- 图论模型
- 储论模型
- 对论模型
- 弈论模型
- 可靠性理论模型

#### 1.2概率论与数理统计模型

- 1.2.1多元分析模型
- 聚类分析
- 主成分分析
- 因子分析
- 判别分析
- 典型相关性分析
- 对应分析
- 多维标度法

#### 1.2.2假设检验模型

- 1.2.3相关分析
- 1.2.4回归分析
- 1.2.5方差分析
- 1.2.6贝叶斯统计模型
- 1.2.7时间序列分析模型
- 1.2.8决策树
- 1.2.9逻辑回归
- 1.3微分方程

传染病模型

人口预测控制模型

灰色预测模型

回归分析预测模型

差分方程模型
马尔科夫预测模型
时间序列模型
插值拟合模型
神经网络模型
系统动力学模型

综合评价与决策方法





### 数据可视化

### 报告输出



#### jupyter 输出文档

安装tex工具

https://nbconvert.readthedocs.io/en/latest/install.html#installing-tex

#### 生成html

jupyter nbconvert notebook.ipynb

#### 生成pdf

jupyter nbconvert --to latex 1-Redcard-Dataset.ipynb
xelatex 1-Redcard-Dataset.tex

或者

jupyter nbconvert notebook.ipynb --to pdf

