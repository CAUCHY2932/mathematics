

<!DOCTYPE html>
<!--[if IE 8]><html class="no-js lt-ie9" lang="en" > <![endif]-->
<!--[if gt IE 8]><!--> <html class="no-js" lang="en" > <!--<![endif]-->
<head>
  <meta charset="utf-8">
  
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  
  <title>bagging和boosting的区别 &mdash; 吕阳阳的日志小屋 1.0 documentation</title>
  

  
  
  
  

  
  <script type="text/javascript" src="../../_static/js/modernizr.min.js"></script>
  
    
      <script type="text/javascript">
          var DOCUMENTATION_OPTIONS = {
              URL_ROOT:'../../',
              VERSION:'1.0',
              LANGUAGE:'None',
              COLLAPSE_INDEX:false,
              FILE_SUFFIX:'.html',
              HAS_SOURCE:  true,
              SOURCELINK_SUFFIX: '.txt'
          };
      </script>
        <script type="text/javascript" src="../../_static/jquery.js"></script>
        <script type="text/javascript" src="../../_static/underscore.js"></script>
        <script type="text/javascript" src="../../_static/doctools.js"></script>
        <script type="text/javascript" src="https://cdnjs.cloudflare.com/ajax/libs/mathjax/2.7.1/MathJax.js?config=TeX-AMS-MML_HTMLorMML"></script>
    
    <script type="text/javascript" src="../../_static/js/theme.js"></script>

    

  
  <link rel="stylesheet" href="../../_static/css/theme.css" type="text/css" />
  <link rel="stylesheet" href="../../_static/pygments.css" type="text/css" />
    <link rel="index" title="Index" href="../../genindex.html" />
    <link rel="search" title="Search" href="../../search.html" /> 
</head>

<body class="wy-body-for-nav">

   
  <div class="wy-grid-for-nav">
    
    <nav data-toggle="wy-nav-shift" class="wy-nav-side">
      <div class="wy-side-scroll">
        <div class="wy-side-nav-search" >
          

          
            <a href="../../index.html" class="icon icon-home"> 吕阳阳的日志小屋
          

          
          </a>

          
            
            
          

          
<div role="search">
  <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
    <input type="text" name="q" placeholder="Search docs" />
    <input type="hidden" name="check_keywords" value="yes" />
    <input type="hidden" name="area" value="default" />
  </form>
</div>

          
        </div>

        <div class="wy-menu wy-menu-vertical" data-spy="affix" role="navigation" aria-label="main navigation">
          
            
            
              
            
            
              <p class="caption"><span class="caption-text">Content:</span></p>
<ul>
<li class="toctree-l1"><a class="reference internal" href="../../chapter00_preface.html">写在前面</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter01_philosophy.html">哲学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter02_data_science.html">数据科学</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter03_spider.html">爬虫</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter04_devops.html">运维和测试</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter05_cs_base.html">计算机科学基础</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter06_think_way.html">思维方式</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter07_offshore_oil_and_gas.html">海洋油气工程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter08_python.html">python</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter09_other.html">其他</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter10_开发流程.html">理解开发流程</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter11_随便聊聊.html">随便聊聊</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter12_big_data.html">大数据</a></li>
<li class="toctree-l1"><a class="reference internal" href="../../chapter13_web.html">python-web</a></li>
</ul>

            
          
        </div>
      </div>
    </nav>

    <section data-toggle="wy-nav-shift" class="wy-nav-content-wrap">

      
      <nav class="wy-nav-top" aria-label="top navigation">
        
          <i data-toggle="wy-nav-top" class="fa fa-bars"></i>
          <a href="../../index.html">吕阳阳的日志小屋</a>
        
      </nav>


      <div class="wy-nav-content">
        
        <div class="rst-content">
        
          















<div role="navigation" aria-label="breadcrumbs navigation">

  <ul class="wy-breadcrumbs">
    
      <li><a href="../../index.html">Docs</a> &raquo;</li>
        
      <li>bagging和boosting的区别</li>
    
    
      <li class="wy-breadcrumbs-aside">
        
            
            <a href="../../_sources/pages_html/s02_数据智能/0209_机器学习.md.txt" rel="nofollow"> View page source</a>
          
        
      </li>
    
  </ul>

  
  <hr/>
</div>
          <div role="main" class="document" itemscope="itemscope" itemtype="http://schema.org/Article">
           <div itemprop="articleBody">
            
  <div class="section" id="baggingboosting">
<h1>bagging和boosting的区别<a class="headerlink" href="#baggingboosting" title="Permalink to this headline">¶</a></h1>
<p>Bagging和Boosting都是将已有的分类或回归算法通过一定方式组合起来，形成一个性能更加强大的分类器，更准确的说这是一种分类算法的组装方法。即将弱分类器组装成强分类器的方法。</p>
<p>首先介绍Bootstraping，即自助法：它是一种有放回的抽样方法（可能抽到重复的样本）。</p>
<p>1、Bagging (bootstrap aggregating)</p>
<p>Bagging即套袋法，其算法过程如下：</p>
<p>A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）</p>
<p>B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）</p>
<p>C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）</p>
<p>2、Boosting</p>
<p>其主要思想是将弱分类器组装成一个强分类器。在PAC（概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。</p>
<p>关于Boosting的两个核心问题：</p>
<p>1）在每一轮如何改变训练数据的权值或概率分布？</p>
<p>通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。</p>
<p>2）通过什么方式来组合弱分类器？</p>
<p>通过加法模型将弱分类器进行线性组合，比如AdaBoost通过加权多数表决的方式，即增大错误率小的分类器的权值，同时减小错误率较大的分类器的权值。</p>
<p>而提升树通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。</p>
<p>3、Bagging，Boosting二者之间的区别</p>
<p>Bagging和Boosting的区别：</p>
<p>1）样本选择上：</p>
<p>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</p>
<p>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。</p>
<p>2）样例权重：</p>
<p>Bagging：使用均匀取样，每个样例的权重相等</p>
<p>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。</p>
<p>3）预测函数：</p>
<p>Bagging：所有预测函数的权重相等。</p>
<p>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。</p>
<p>4）并行计算：</p>
<p>Bagging：各个预测函数可以并行生成</p>
<p>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。</p>
<p>4、总结</p>
<p>这两种方法都是把若干个分类器整合为一个分类器的方法，只是整合的方式不一样，最终得到不一样的效果，将不同的分类算法套入到此类算法框架中一定程度上会提高了原单一分类器的分类效果，但是也增大了计算量。</p>
<p>下面是将决策树与这些算法框架进行结合所得到的新的算法：</p>
<p>1）Bagging + 决策树 = 随机森林</p>
<p>2）AdaBoost + 决策树 = 提升树</p>
<p>3）Gradient Boosting + 决策树 = GBDT</p>
</div>
<div class="section" id="id1">
<h1>集成学习法<a class="headerlink" href="#id1" title="Permalink to this headline">¶</a></h1>
<blockquote>
<div>https://www.cnblogs.com/zongfa/p/9304353.html</div></blockquote>
<p>弱分类器组成强分类器</p>
<p>在机器学习的有监督学习算法中，我们的目标是学习出一个稳定的且在各个方面表现都较好的模型，但实际情况往往不这么理想，有时我们只能得到多个有偏好的模型（弱监督模型，在某些方面表现的比较好）。集成学习就是组合这里的多个弱监督模型以期得到一个更好更全面的强监督模型，集成学习潜在的思想是即便某一个弱分类器得到了错误的预测，其他的弱分类器也可以将错误纠正回来。</p>
<p>集成方法是将几种机器学习技术组合成一个预测模型的元算法，以达到减小方差（bagging）、偏差（boosting）或改进预测（stacking）的效果。</p>
<p>集成学习在各个规模的数据集上都有很好的策略。</p>
<p>数据集大：划分成多个小数据集，学习多个模型进行组合</p>
<p>数据集小：利用Bootstrap方法进行抽样，得到多个数据集，分别训练多个模型再进行组合</p>
<p>集合方法可分为两类：</p>
<div class="highlight-default notranslate"><div class="highlight"><pre><span></span>序列集成方法，其中参与训练的基础学习器按照顺序生成（例如 AdaBoost）。序列方法的原理是利用基础学习器之间的依赖关系。通过对之前训练中错误标记的样本赋值较高的权重，可以提高整体的预测效果。
并行集成方法，其中参与训练的基础学习器并行生成（例如 Random Forest）。并行方法的原理是利用基础学习器之间的独立性，通过平均可以显著降低错误。
</pre></div>
</div>
<p>总结一下，集成学习法的特点：</p>
<p>①  将多个分类方法聚集在一起，以提高分类的准确率。</p>
<p>（这些算法可以是不同的算法，也可以是相同的算法。）</p>
<p>②  集成学习法由训练数据构建一组基分类器，然后通过对每个基分类器的预测进行投票来进行分类</p>
<p>③  严格来说，集成学习并不算是一种分类器，而是一种分类器结合的方法。</p>
<p>④  通常一个集成分类器的分类性能会好于单个分类器</p>
<p>⑤  如果把单个分类器比作一个决策者的话，集成学习的方法就相当于多个决策者共同进行一项决策。</p>
<p>自然地，就产生两个问题：</p>
<p>1）怎么训练每个算法？</p>
<p>2）怎么融合每个算法？</p>
<p>这篇博客介绍一下集成学习的几个方法：Bagging，Boosting以及Stacking。</p>
<p>1、Bagging（bootstrap aggregating，装袋）</p>
<p>Bagging即套袋法，先说一下bootstrap，bootstrap也称为自助法，它是一种有放回的抽样方法，目的为了得到统计量的分布以及置信区间，其算法过程如下：</p>
<p>A）从原始样本集中抽取训练集。每轮从原始样本集中使用Bootstraping的方法抽取n个训练样本（在训练集中，有些样本可能被多次抽取到，而有些样本可能一次都没有被抽中）。共进行k轮抽取，得到k个训练集。（k个训练集之间是相互独立的）</p>
<p>B）每次使用一个训练集得到一个模型，k个训练集共得到k个模型。（注：这里并没有具体的分类算法或回归方法，我们可以根据具体问题采用不同的分类或回归方法，如决策树、感知器等）</p>
<p>C）对分类问题：将上步得到的k个模型采用投票的方式得到分类结果；对回归问题，计算上述模型的均值作为最后的结果。（所有模型的重要性相同）</p>
<p>由此，总结一下bagging方法：</p>
<p>①  Bagging通过降低基分类器的方差，改善了泛化误差
　　②  其性能依赖于基分类器的稳定性；如果基分类器不稳定，bagging有助于降低训练数据的随机波动导致的误差；如果稳定，则集成分类器的误差主要由基分类器的偏倚引起
　　③  由于每个样本被选中的概率相同，因此bagging并不侧重于训练数据集中的任何特定实例</p>
<p>常用的集成算法类是随机森林。</p>
<p>在随机森林中，集成中的每棵树都是由从训练集中抽取的样本（即 bootstrap 样本）构建的。另外，与使用所有特征不同，这里随机选择特征子集，从而进一步达到对树的随机化目的。</p>
<p>因此，随机森林产生的偏差略有增加，但是由于对相关性较小的树计算平均值，估计方差减小了，导致模型的整体效果更好。</p>
<p>2、Boosting</p>
<p>其主要思想是将弱分类器组装成一个强分类器。在PAC（probably approximately correct，概率近似正确）学习框架下，则一定可以将弱分类器组装成一个强分类器。</p>
<p>关于Boosting的两个核心问题：</p>
<p>1）在每一轮如何改变训练数据的权值或概率分布？</p>
<p>通过提高那些在前一轮被弱分类器分错样例的权值，减小前一轮分对样例的权值，来使得分类器对误分的数据有较好的效果。</p>
<p>2）通过什么方式来组合弱分类器？</p>
<p>通过加法模型将弱分类器进行线性组合，比如：</p>
<p>AdaBoost（Adaptive boosting）算法：刚开始训练时对每一个训练例赋相等的权重，然后用该算法对训练集训练t轮，每次训练后，对训练失败的训练例赋以较大的权重，也就是让学习算法在每次学习以后更注意学错的样本，从而得到多个预测函数。通过拟合残差的方式逐步减小残差，将每一步生成的模型叠加得到最终模型。</p>
<p>GBDT（Gradient Boost Decision Tree)，每一次的计算是为了减少上一次的残差，GBDT在残差减少（负梯度）的方向上建立一个新的模型。</p>
<p>3、Stacking</p>
<p>Stacking方法是指训练一个模型用于组合其他各个模型。首先我们先训练多个不同的模型，然后把之前训练的各个模型的输出为输入来训练一个模型，以得到一个最终的输出。理论上，Stacking可以表示上面提到的两种Ensemble方法，只要我们采用合适的模型组合策略即可。但在实际中，我们通常使用logistic回归作为组合策略。</p>
<p>如下图，先在整个训练数据集上通过bootstrap抽样得到各个训练集合，得到一系列分类模型，然后将输出用于训练第二层分类器。</p>
<p>二、Bagging，Boosting二者之间的区别</p>
<p>1、Bagging和Boosting的区别：</p>
<p>1）样本选择上：</p>
<p>Bagging：训练集是在原始集中有放回选取的，从原始集中选出的各轮训练集之间是独立的。</p>
<p>Boosting：每一轮的训练集不变，只是训练集中每个样例在分类器中的权重发生变化。而权值是根据上一轮的分类结果进行调整。</p>
<p>2）样例权重：</p>
<p>Bagging：使用均匀取样，每个样例的权重相等</p>
<p>Boosting：根据错误率不断调整样例的权值，错误率越大则权重越大。</p>
<p>3）预测函数：</p>
<p>Bagging：所有预测函数的权重相等。</p>
<p>Boosting：每个弱分类器都有相应的权重，对于分类误差小的分类器会有更大的权重。</p>
<p>4）并行计算：</p>
<p>Bagging：各个预测函数可以并行生成</p>
<p>Boosting：各个预测函数只能顺序生成，因为后一个模型参数需要前一轮模型的结果。</p>
<p>2、决策树与这些算法框架进行结合所得到的新的算法：</p>
<p>1）Bagging + 决策树 = 随机森林</p>
<p>2）AdaBoost + 决策树 = 提升树</p>
<p>3）Gradient Boosting + 决策树 = GBDT</p>
<p>参考博文：</p>
<p>【1】集成学习总结 &amp; Stacking方法详解  https://blog.csdn.net/willduan1/article/details/73618677</p>
<p>【2】Bagging和Boosting 概念及区别  https://www.cnblogs.com/liuwu265/p/4690486.html</p>
<p>【3】集成学习法之bagging方法和boosting方法 https://blog.csdn.net/qq_30189255/article/details/51532442</p>
<p>【4】机器学习中的集成学习（Ensemble Learning)  http://baijiahao.baidu.com/s?id=1590266955499942419&amp;wfr=spider&amp;for=pc</p>
<p>【5】简单易学的机器学习算法——集成方法(Ensemble Method) https://blog.csdn.net/google19890102/article/details/46507387</p>
</div>


           </div>
           
          </div>
          <footer>
  

  <hr/>

  <div role="contentinfo">
    <p>
        &copy; Copyright 2019@young

    </p>
  </div>
  Built with <a href="http://sphinx-doc.org/">Sphinx</a> using a <a href="https://github.com/rtfd/sphinx_rtd_theme">theme</a> provided by <a href="https://readthedocs.org">Read the Docs</a>. 

</footer>

        </div>
      </div>

    </section>

  </div>
  


  <script type="text/javascript">
      jQuery(function () {
          SphinxRtdTheme.Navigation.enable(true);
      });
  </script>

  
  
    
   

</body>
</html>